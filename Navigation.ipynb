{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana_Windows_x86_64/Banana.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.l1 = nn.Linear(state_size,150)\n",
    "        self.l2 = nn.Linear(150,50)\n",
    "        \n",
    "        self.adv = nn.Linear(50, action_size)\n",
    "        \n",
    "        self.val = nn.Linear(50,1)\n",
    "    def forward(self, state):\n",
    "        state = F.relu(self.l1(state))\n",
    "        state = F.relu(self.l2(state))\n",
    "        advantage = self.adv(state)\n",
    "        state = self.val(state) + (advantage - torch.mean(advantage))\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "  def __init__(self, state_space, action_space, gamma):\n",
    "    self.Q_Net = QNetwork(state_space,action_space)\n",
    "    self.Q_Target = deepcopy(self.Q_Net)\n",
    "    self.state_space = state_space\n",
    "    self.action_space = action_space\n",
    "    self.gamma = gamma\n",
    "    self.optimizer = optim.Adam(self.Q_Net.parameters(), lr=0.001)\n",
    "    self.scheduler = optim.lr_scheduler.ExponentialLR(self.optimizer, 0.998)#Multiplies learning rate by 0.998 every game\n",
    "    self.epsilon = 30\n",
    "    \n",
    "  def get_action(self, s):\n",
    "    #picks action using the epsilon greedy algorithm\n",
    "    if randint(0,100) > self.epsilon:\n",
    "      Q_Vals = self.Q_Net(torch.from_numpy(s).float())\n",
    "      return torch.argmax(Q_Vals)\n",
    "    else:\n",
    "      return randint(0,self.action_space - 1)\n",
    "\n",
    "  def train_state(self, s0, s1, action, reward, done):\n",
    "    Q_Vals = self.Q_Net(torch.from_numpy(s0).float())\n",
    "    Target_Q_Vals = self.Q_Target(torch.from_numpy(s1).float())\n",
    "    Next_Q_Vals = self.Q_Net(torch.from_numpy(s1).float())\n",
    "    #Uses Double DQN\n",
    "    if done:\n",
    "      update_loss = (Q_Vals[action] - reward)**2\n",
    "    else:\n",
    "      update_loss = (Q_Vals[action] - self.gamma * Target_Q_Vals[torch.argmax(Next_Q_Vals)] - reward)**2\n",
    "    return update_loss\n",
    "  def update_target(self):\n",
    "    self.Q_Target = deepcopy(self.Q_Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(37,4, 0.98)\n",
    "\n",
    "num_episodes = 10000\n",
    "batch_size = 16\n",
    "update_frequency = 4\n",
    "T_update_frequency = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1\n",
      "1.72\n",
      "3.2\n",
      "5.5\n",
      "6.78\n",
      "9.34\n",
      "10.08\n",
      "10.94\n",
      "10.06\n",
      "11.62\n",
      "12.58\n",
      "12.74\n",
      "12.56\n",
      "12.98\n",
      "13.22\n",
      "14.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-b68bb085a930>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m#num = randint(0,len(state_list)-1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnext_state_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mprob_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mprob_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-10931edddbd0>\u001b[0m in \u001b[0;36mtrain_state\u001b[1;34m(self, s0, s1, action, reward, done)\u001b[0m\n\u001b[0;32m     22\u001b[0m       \u001b[0mupdate_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mQ_Vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m       \u001b[0mupdate_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mQ_Vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mTarget_Q_Vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNext_Q_Vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m       \u001b[1;31m#update_loss = (Q_Vals[action] - self.gamma * torch.max(Target_Q_Vals) - reward)**2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mupdate_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda4\\envs\\drlnd\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(input, dim, keepdim)\u001b[0m\n\u001b[0;32m    392\u001b[0m     \"\"\"\n\u001b[0;32m    393\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_argmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_argmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Trains agent\n",
    "state_list = list()\n",
    "action_list = list()\n",
    "done_list = list()\n",
    "reward_list = list()\n",
    "next_state_list = list()\n",
    "indexes = list()\n",
    "prob_list = list()\n",
    "b =  0.6\n",
    "a = 0.9\n",
    "\n",
    "score_list = list()\n",
    "score = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "  done = False\n",
    "  env_info = env.reset(train_mode=True)[brain_name]\n",
    "  s0 = env_info.vector_observations[0]       \n",
    "  ep_loss = 0\n",
    "  for frame in range(50000):\n",
    "    action = agent.get_action(s0)#Gets action\n",
    "    env_info = env.step(int(action))[brain_name]#Acts according to chosen action\n",
    "\n",
    "    state = env_info.vector_observations[0]   \n",
    "    reward = env_info.rewards[0]                   \n",
    "    done = env_info.local_done[0] \n",
    "    score += reward\n",
    "    \n",
    "    #Saves states, actions, rewards, and wether is done\n",
    "    state_list.append(s0)\n",
    "    action_list.append(action)\n",
    "    reward_list.append(reward)\n",
    "    next_state_list.append(state)\n",
    "    done_list.append(done)\n",
    "    indexes.append(len(state_list)-1)\n",
    "    if len(prob_list) > 10:\n",
    "        prob_list.append(np.mean(prob_list))\n",
    "    else:\n",
    "        prob_list.append(20)\n",
    "\n",
    "    s0 = state\n",
    "    #Calculates loss and update network\n",
    "    if frame%update_frequency == 0 and episode+frame > 0:\n",
    "      loss = 0\n",
    "      #Implements prioritized experience replay\n",
    "      ind = np.random.choice(np.array(indexes),size = batch_size, p = (np.array(prob_list)**0.9/sum(np.array(prob_list)**0.9)))\n",
    "      for i in ind:\n",
    "        num = i\n",
    "        #calculates loss\n",
    "        d_loss =  agent.train_state(state_list[num],next_state_list[num],action_list[num],reward_list[num],done_list[num])\n",
    "        loss += float(sum(prob_list)/prob_list[i] * 1/batch_size)**b * d_loss \n",
    "        #Updates probability of learning from same state\n",
    "        prob_list[i] = np.sqrt(d_loss.detach().numpy()) + 1\n",
    "        ep_loss += d_loss\n",
    "      #Takes training step\n",
    "      agent.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      agent.optimizer.step()\n",
    "    if done:\n",
    "      break\n",
    "  score_list.append(score)\n",
    "  score = 0\n",
    "  #Anneals learning rate, helps speed up training by allowing for high initial learning rate\n",
    "  agent.scheduler.step()\n",
    "  #Slowly lowers epsilon to 0 to reduce probability of random actions\n",
    "  agent.epsilon += -0.03\n",
    "  if b < 1:\n",
    "    b = b + 0.0003\n",
    "  if a > 0:\n",
    "    a = a - 0.0005\n",
    "  #Sets target net equal to Q net every T_update_frequency games\n",
    "  if episode%T_update_frequency == 0 and episode > 0:\n",
    "    agent.update_target()\n",
    "  if episode%50 == 0 and episode > 0:\n",
    "    print(sum(score_list[-50:])/50)\n",
    "  #Only keeps last 10000 states\n",
    "  if len(state_list) > 10000:\n",
    "    state_list = state_list[-10000:]\n",
    "    action_list = action_list[-10000:]\n",
    "    reward_list = reward_list[-10000:]\n",
    "    done_list = done_list[-10000:]\n",
    "    next_state_list = next_state_list[-10000:]\n",
    "    indexes = indexes[:10000]\n",
    "    prob_list = prob_list[-10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1ac921a4713e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Plots learning curve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'score_list' is not defined"
     ]
    }
   ],
   "source": [
    "#Plots learning curve\n",
    "plt.plot(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves model weights\n",
    "torch.save(agent.Q_Net.state_dict(), 'Navigation_Q_Net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.0\n"
     ]
    }
   ],
   "source": [
    "#Runs game with trained model and prints out score\n",
    "agent = Agent(37,4, 0.98)\n",
    "#Loads model weight\n",
    "agent.Q_Net.load_state_dict(torch.load('Navigation_Q_Net')) \n",
    "#Sets epsilon to 0 since not training\n",
    "agent.epsilon = 0\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "s0 = env_info.vector_observations[0]       \n",
    "score = 0\n",
    "for frame in range(50000):\n",
    "    action = agent.get_action(s0)\n",
    "    env_info = env.step(int(action))[brain_name] \n",
    "    state = env_info.vector_observations[0]   \n",
    "    reward = env_info.rewards[0]         \n",
    "    score += reward\n",
    "    done = env_info.local_done[0] \n",
    "    s0 = state\n",
    "    if done:\n",
    "        break\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BananaBrain': <unityagents.brain.BrainInfo at 0x20de3887630>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
